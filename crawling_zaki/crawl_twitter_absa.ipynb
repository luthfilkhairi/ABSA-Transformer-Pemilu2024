{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBT9RFq0TbIv",
        "outputId": "7b0d4eb8-f0d7-4650-9689-d7e6c7f96463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting snscrape\n",
            "  Downloading snscrape-0.7.0.20230622-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from snscrape) (2.32.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from snscrape) (5.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from snscrape) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from snscrape) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->snscrape) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->snscrape) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->snscrape) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->snscrape) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->snscrape) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->snscrape) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->snscrape) (1.7.1)\n",
            "Downloading snscrape-0.7.0.20230622-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snscrape\n",
            "Successfully installed snscrape-0.7.0.20230622\n"
          ]
        }
      ],
      "source": [
        "!pip install snscrape pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Kx0G34yXSSZM",
        "outputId": "8dff0c19-65ef-4466-9a3f-8cffde8a6a7c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'FileFinder' object has no attribute 'find_module'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2756216718.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# -----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/snscrape/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0m_import_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/snscrape/modules/__init__.py\u001b[0m in \u001b[0;36m_import_modules\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mmoduleNameWithoutPrefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoduleName\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprefixLen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0m__all__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleNameWithoutPrefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleNameWithoutPrefix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'FileFinder' object has no attribute 'find_module'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "\n",
        "# -----------------------------\n",
        "# KONFIGURASI DASAR\n",
        "# -----------------------------\n",
        "OUTPUT_DIR = \"data/crawling_2024\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "DATE_START = \"2023-09-01\"\n",
        "DATE_END = \"2024-05-31\"\n",
        "MAX_TWEETS = 1000  # target per aspek\n",
        "\n",
        "# -----------------------------\n",
        "# KEYWORD PER ASPEK\n",
        "# -----------------------------\n",
        "aspect_keywords = {\n",
        "    \"integritas\": [\n",
        "        '\"bersih dari korupsi\"', '\"tidak korupsi\"', '\"pemimpin jujur\"', '\"integritas tinggi\"', '\"amanah\"'\n",
        "    ],\n",
        "    \"kapabilitas\": [\n",
        "        '\"kompeten\"', '\"kapabilitas\"', '\"berpengalaman memimpin\"', '\"cerdas\"', '\"profesional\"'\n",
        "    ],\n",
        "    \"empati\": [\n",
        "        '\"peduli rakyat\"', '\"turun ke rakyat\"', '\"bantu korban\"', '\"dekat dengan rakyat\"'\n",
        "    ],\n",
        "    \"akseptabilitas\": [\n",
        "        '\"disukai rakyat\"', '\"diterima masyarakat\"', '\"elektabilitas tinggi\"', '\"popularitas kandidat\"'\n",
        "    ],\n",
        "    \"kontinuitas\": [\n",
        "        '\"lanjutkan program\"', '\"keberlanjutan pembangunan\"', '\"visi jangka panjang\"', '\"melanjutkan kerja\"'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# -----------------------------\n",
        "# FUNGSI CRAWLING\n",
        "# -----------------------------\n",
        "def crawl_tweets(keyword_query, max_tweets=500):\n",
        "    tweets_data = []\n",
        "    query = f\"{keyword_query} lang:id since:{DATE_START} until:{DATE_END}\"\n",
        "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
        "        if i >= max_tweets:\n",
        "            break\n",
        "        tweets_data.append({\n",
        "            \"date\": tweet.date,\n",
        "            \"username\": tweet.user.username,\n",
        "            \"content\": tweet.rawContent,\n",
        "            \"replyCount\": tweet.replyCount,\n",
        "            \"retweetCount\": tweet.retweetCount,\n",
        "            \"likeCount\": tweet.likeCount,\n",
        "            \"url\": tweet.url\n",
        "        })\n",
        "    return tweets_data\n",
        "\n",
        "# -----------------------------\n",
        "# PROSES CRAWLING PER ASPEK\n",
        "# -----------------------------\n",
        "for aspect, keywords in aspect_keywords.items():\n",
        "    print(f\"\\nüîç Mengambil tweet untuk aspek: {aspect}\")\n",
        "    all_tweets = []\n",
        "    for kw in tqdm(keywords, desc=f\"Aspek {aspect}\"):\n",
        "        tweets = crawl_tweets(kw, max_tweets=MAX_TWEETS // len(keywords))\n",
        "        all_tweets.extend(tweets)\n",
        "\n",
        "    # Simpan ke CSV\n",
        "    df = pd.DataFrame(all_tweets)\n",
        "    df[\"aspect_target\"] = aspect\n",
        "    csv_path = os.path.join(OUTPUT_DIR, f\"tweets_{aspect}.csv\")\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"‚úÖ Disimpan: {csv_path} ({len(df)} tweet)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlGaxnsj_5rK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
